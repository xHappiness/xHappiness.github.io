<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[线性回归分享KeyWord]]></title>
    <url>%2F2018%2F05%2F10%2FKeyWord%2F</url>
    <content type="text"><![CDATA[用例子解释 什么是 最小二乘法 人们会选择距离最进行的线? 引入 线性回归 概念 介绍 多元线性回归 多项式回归 损失函数 J(thta) y = x 为例 一元 抛物线 二元 等高线 多元变量回归的损失函数 梯度下降 随机梯度下降 为啥 前面 添加 二分之一 m分之一 极值点 一定是最优解吗？ 归一化 作用 重新强调 快速收敛 学习率（步长） 不能过高 不要过低 矩阵的逆的求解方法 x0 可以通过数学推到得到。。。。 m比n多时 仅仅求得其中一个解 正则化 过拟合 交叉验证 如果我们没有足够的数据集（训练集）去约束这个变量过多的模型，那么就会发生过拟合。过度拟合的问题通常发生在变量（特征）过多的时候。这种情况下训练出的方程总是能很好的拟合训练数据，也就是说，我们的代价函数可能非常接近于 0 或者就为 0。 方法一：尽量减少选取变量的数量 具体而言，我们可以人工检查每一项变量，并以此来确定哪些变量更为重要，然后，保留那些更为重要的特征变量。至于，哪些变量应该舍弃，我们以后在讨论，这会涉及到模型选择算法，这种算法是可以自动选择采用哪些特征变量，自动舍弃不需要的变量。这类做法非常有效，但是其缺点是当你舍弃一部分特征变量时，你也舍弃了问题中的一些信息。例如，也许所有的特征变量对于预测房价都是有用的，我们实际上并不想舍弃一些信息或者说舍弃这些特征变量。 方法二：正则化 正则化中我们将保留所有的特征变量，但是会减小特征变量的数量级（参数数值的大小θ(j)）。 这个方法非常有效，当我们有很多特征变量时，其中每一个变量都能对预测产生一点影响。正如我们在房价预测的例子中看到的那样，我们可以有很多特征变量，其中每一个变量都是有用的，因此我们不希望把它们删掉，这就导致了正则化概念的发生。 如果特征比样本点还多（n &gt; m）时，也就是说输入数据的矩阵X不是满秩矩阵（满秩矩阵是求逆矩阵的充分必要条件，一般的回归问题中X通常是列满秩，而当n &gt; m时，不可能是列满秩。当不能满足列满秩时，也就不能对（X^T * X）^ I求逆。） 容易出现欠拟合 一个点为例 理想丰满抵不过现实残酷，求解正规方程需要求矩阵的逆，学过线性代数的同学都知道不是所有的矩阵都能求逆，而且求逆也是一个很花时间的过程（(XTX)−1 的求解复杂度为 o(n3)），在矩阵很大时，求逆的计算量往往大于梯度下降算法，因此实际问题还是以梯度下降这种迭代方法为主，当研究对象的特征较小时可以考虑使用正规方程求解。 所以过拟合有两种原因：训练集和测试机特征分布不一致（白天鹅黑天鹅）或者模型太过复杂（记住了每道题）而样本量不足 过拟合(over-fitting)指的是你建模的时候用了过多的参数，一般地，我们建模的时候要让参数个数尽可能的少。然后，只要你的模型参数有很多，做出来的R平方也可以很好看。所以，为了避免这种情况，就有了Akaike information criterion和Bayesian information criterion等判定准则，大致原理是你用的参数越少，这个值则越小，模型越棒。多重共线性(multicollinearity)指的是你建模的时候，解释变量之间有高度相关性。这种情况的表现就是：你在做test statistics时候，单个解释变量不显著，整体确实显著的。这样一一来你的模型的参数估计是没有意义。所以，我们经常要判断这种关系存不存在，有的方法主流的就是首先在variable selection用stepwise regression把这种关系先杜绝掉，或者是VIF来判断，方差膨胀因子表达式为：VIF_i=1/（1-R_i^2)。其中Ri为自变量xi对其余自变量作回归分析的复相关系数。当VIF_i很大时，表明自变量间存在多重共线性。一般大于10就说有多重共线性存在。 123456789101112131415161718&lt;!DOCTYPE HTML&gt;&lt;html&gt;&lt;head&gt; &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html;charset=utf-8;&quot;/&gt; &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge,chrome=1&quot; /&gt; &lt;meta name=&quot;robots&quot; content=&quot;all&quot; /&gt; &lt;meta name=&quot;robots&quot; content=&quot;index,follow&quot;/&gt; &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://qzone.qq.com/gy/404/style/404style.css&quot;&gt;&lt;/head&gt;&lt;body&gt; &lt;script type=&quot;text/plain&quot; src=&quot;http://www.qq.com/404/search_children.js&quot; charset=&quot;utf-8&quot; homePageUrl=&quot;/&quot; homePageName=&quot;回到我的主页&quot;&gt; &lt;/script&gt; &lt;script src=&quot;https://qzone.qq.com/gy/404/data.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt; &lt;script src=&quot;https://qzone.qq.com/gy/404/page.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt;]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Linear Regression</tag>
      </tags>
  </entry>
</search>
